---
title: "Activity Prediction Model from personal fitness device data"
author: "Syed Abdullah Hasan"
date: "8/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = TRUE, 
                      fig.align = 'center', echo = F, message = F, include = T,
                      warning = F)

```

```{r Load_dependencies}
library (caret); library (ggplot2); library (kernlab); library (Hmisc)
library (splines); library (dplyr); library (reshape2); library (MASS)
library (GGally); library (scales); library (kableExtra); library (corrplot)
library (rattle)

train_URL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_URL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_raw<- read.csv(train_URL)
test_raw<-read.csv(test_URL)
```

# Executive Summary
# 1. Pre-processing Methodology
The following transformations have been made to clean and prepare the data for processing:
1. Outcome variable is converted to a numeric factor variable with levels A-E corresponding to numbers 1-5. This will facilitate exploratory analysis and model building.
1. Metadata variables corresponding to columns 1-7 have been removed from the data sets.
2. Character variables other than the outcome variable are all converted to numeric format.
3. Some variables include missing values in over 97% of observations - any variables with over 90% of values missing are excluded.
4. Covariates with near-zero variation have been removed.
5. Covariates with high pair-wise correlations are also removed.
6. All remaining variables are standardized.

Subsequently, 30% of the the training data is partitioned into a validation set whereas the remainder is split between training (70%) and test (30%) sets. 

``` {r}
# Separate and factorize outcome variable
train_clean<-train_raw[,-dim(train_raw)[2]]
classe<-train_raw[,dim(train_raw)[2]]
classe<-as.factor(classe); levels(classe)<-c(1,2,3,4,5)
classe<-as.numeric(classe)

# Remove metadata
train_clean<-train_clean[,-c(1:7)]

# Convert character variables to numeric
chrVar<-grep("TRUE",sapply(train_clean, function (x) {class(x)=="character"}))
train_clean[,chrVar] <- as.numeric(unlist(train_clean[,chrVar]))

# Identify and remove variables with over 90% NA values
t_na<- data.frame(colMeans(is.na(train_clean)))
na <- which(t_na>0.9)

# Remove covariates with near zero variation
nzv <-nearZeroVar(train_clean[,-na])
nzv <- names(train_clean[,-na])[nzv]

# Remove covariates with high pair-wise correlations
hcv <- findCorrelation(cor(train_clean[,-na]), cutoff=0.70)
hcv <- names(train_clean[,-na])[hcv]

# Recombine outcome variable
exclude <- unique(c(nzv,hcv,row.names(t_na)[na]))
train_clean <- data.frame(classe,
                          train_clean[,-which(names(train_clean)%in%exclude)])

# Split data into train, test and validation sets
set.seed (12345)
inBuild <- createDataPartition(y=train_clean$classe,p=0.7,list=F)
validation <- train_clean[-inBuild,]; buildData<- train_clean[inBuild,]
inTrain <- createDataPartition (y=buildData$classe,p=0.7, list=F)
train <- train_clean[inTrain,]; test<-train_clean[-inTrain,]
```

## 1.3. Outliers
## 1.4. Skewed Variables
## 1.5. Standardization
## 1.7. Covariate Creation

# 2. Exploratory Analysis
## 2.1. Individual Predictors
``` {r}
## Scatter plots
par (mfrow=c(8,7))
lapply (1:dim(train)[2], function(x) {
        plot (train[,x],col=train$classe)
})
```
## 2.2. Correlation Plot
``` {r, echo=F}
## Correlation matrix
cor_mat <- cor(train)
corrplot(cor_mat, order = "FPC", method = "color", type = "upper", 
         tl.cex = 0.3, tl.col = rgb(0, 0, 0))

names(train)[abs(cor_mat[1,-1])>0.15]
```

``` {r}
hcv <- findCorrelation(cor(train), cutoff=0.75)
names(train)[hcv]
```

## 2.3 K-Means Clustering Analysis
``` {r}
kmc <- kmeans(train[,-1],centers=5)
cluster<-kmc$cluster
qplot (cor_vars[1],cor_vars[2], data=train, colour=cluster)
```
## 2.4 Principal Components Analysis

# 3. Model Selection Methodology
## 3.1. Muti-variate Regression
## 3.2. Random Forests
## 3.3. Bagging
## 3.4. Boosting
## 3.5. Linear Discriminant Analysis
## 3.6. Naive Bayes
## 3.7. Regularized Regression
## 3.8. Unsupervised prediction
## 3.9. Combining Predictors
## 3.10. Final Model
## 3.11. In-Sample Error
## 3.12. Out-of Sample Error

\newpage 
# Appendix
## 1. Pre-processing
``` {r}
covariates<-names(train)
features<-grep("^avg|^var|^stddev|^min|^max|^skewness|^kurtosis",
               covariates, value=T)
variables<-covariates[-grep("^avg|^var|^stddev|^min|^max|^skewness|^kurtosis",
                            covariates)]
dim (train); length (features); length (variables)

t_na<- data.frame(sapply(train, function(x) {sum(is.na(x))}))
t_na/dim(train)[1]
prop.table (t_na)

t_class <-table(sapply(train,class))
t_class
prop.table (t_class)
summary (train)

table(is.na(train))

cov_chr<-covariates[sapply(train, function (x) {class(x)=="character"})]
cov_num<-covariates[sapply(train, function (x) {class(x)=="numeric"})]
cov_int<-covariates[sapply(train, function (x) {class(x)=="integer"})]

colMeans(is.na(train))

par (mfrow=c(7,7))
lapply (cov_chr, function(x) {qplot(x, data=train, colour=classe,stat="count",
                                    geom=c("histogram"))})
```

## 2. Exploratory Analysis  
## 3. Model Selection 
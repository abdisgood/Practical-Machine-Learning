---
title: "Activity Prediction Model from personal fitness device data"
author: "Syed Abdullah Hasan"
date: "8/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = TRUE, 
                      fig.align = 'center', echo = F, message = F, include = T,
                      warning = F)

```

```{r Load_dependencies, include=F}
library (caret); library (ggplot2); library (kernlab); library (Hmisc)
library (splines); library (dplyr); library (reshape2); library (MASS)
library (GGally); library (scales); library (kableExtra); library (corrplot)
library (rattle); library (dendextend); library (factoextra)
library (data.table); library (randomForest); library (randomForestSRC); 
library (doParallel)

train_URL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_URL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_raw<- read.csv(train_URL)
test_raw<-read.csv(test_URL)
```

# Executive Summary 
This project examines data from personal fitness devices to develop a machine learning model and predict outcomes for a test set. The data used herein covers five cases of exercise being conducted by participants. Exploratory analysis is used to determine preprocessing requirements for the data set and develop preliminary classification based on clustering techniques. Subsequently, several machine learning models have been developed of which random forests generate the best results as evaluated on in-sample and out-of-sample predictive accuracy. Predictions on twenty cases of test data are subsequantly provided.

### 1. Pre-processing Methodology
The raw data set includes a total of `r dim(train_raw)[1]` observations for `r dim(train_raw)[2]` variables. The outcome variable includes `r length(unique(train_raw$classe))` levels, for which observations are distributed as follows:
```{r outcome_table}
obs<- data.frame(table(train_raw$classe))
names(obs) <- c("Outcome", "Observations")
obs$Observations <- format(obs$Observations,big.mark = ",")

obs %>% kbl %>% kable_styling (bootstrap_options = c("striped",
                                                     "hover",
                                                     "condensed"),
                               font_size = 10)
       
```

The following transformations have been made to clean and prepare the data for processing: 

1. Outcome variable is converted to a numeric factor variable with levels A-E corresponding to numbers 1-5. This will facilitate exploratory analysis and model building.
2. Metadata variables corresponding to columns 1-7 have been removed from the data sets. 
3. Character variables other than the outcome variable are all converted to numeric format. 
4. Some variables include missing values in over 97% of observations - any variables with over 90% of values missing are excluded. 
5. Covariates with near-zero variation have been removed. 
6. Covariates with high pair-wise correlations are also removed. 

Subsequently, the data is is partitioned into training (70%) and test (30%) sets. 

``` {r, include=F}
# Separate and factorize outcome variable
train_clean<-train_raw[,-dim(train_raw)[2]]
classe<-train_raw[,dim(train_raw)[2]]
classe<-as.factor(classe); levels(classe)<-c(1,2,3,4,5)

# Remove metadata
train_clean<-train_clean[,-c(1:7)]

# Convert character variables to numeric
chrVar<-grep("TRUE",sapply(train_clean, function (x) {class(x)=="character"}))
train_clean[,chrVar] <- as.numeric(unlist(train_clean[,chrVar]))

# Identify variables with over 90% NA values
t_na<- data.frame(colMeans(is.na(train_clean)))
na <- which(t_na>0.9)

# Identify covariates with near zero variation
nzv <-nearZeroVar(train_clean[,-na])
nzv <- names(train_clean[,-na])[nzv]

# Identify covariates with high pair-wise correlations
hcv <- findCorrelation(cor(train_clean[,-na]), cutoff=0.70)
hcv <- names(train_clean[,-na])[hcv]

# Remove variables, standardize and recombine outcome variable
exclude <- unique(c(nzv,hcv,row.names(t_na)[na]))
covariates<- train_clean[,-which(names(train_clean)%in%exclude)]
preObj <- preProcess(covariates, method=c("BoxCox"))
std_cov <- predict (preObj, covariates)
train_clean <- data.frame(classe, covariates)

# Split data into train, test and validation sets
set.seed (568943)
inTrain <- createDataPartition (y=train_clean$classe, p=0.7, list=F)
test <- train_clean [-inTrain,]; train <- train_clean [inTrain,]
```
The final distribution of observations between data sets is as follows:
``` {r}
dist<- data.frame("Dataset" = c("Test","Train"),
                  "Observations" = c(dim(test)[1],
                                     dim(train)[1]))
dist$Observations<-format(dist$Observations,big.mark=",")
dist %>% kbl %>% kable_styling (bootstrap_options = c("striped",
                                                     "hover",
                                                     "condensed"),
                               font_size = 10)
```

### 2. Exploratory Analysis 
```{r, include =F}
train_n <- data.frame(as.numeric(train$classe),train[,-1])
corVars<- names(train_n)[abs(cor(train_n)[1,-1])>0.15]
```
A basic exploratory analysis of the predictors shall include scatter plots, histograms and a correlation plot (included in the Appendix). The following issues can be identified from the plots:

1. *Outlier variables* - Several of the predictors have a high number of outlier variables which are likely to distort correlations and affect model building. This can be addressed by standardizing variables when pre-processing data into the model.
2. *Skewed variables* - Most of the predictors do not conform to a normal distribution. We can therefore attempt to standardize using a Box-Cox transformation. Even after the transformation, qq-plots indicate that the normality assumption may not be valid. However, the standardization should facilitate model building.
3. *Strong pair-wise correlation* - The correlation matrix for training data shows that most variables have weak correlation with the outcome variable, ranging from `r max(cor(train_n)[1,-1])` to `r min(cor(train_n)[1,-1])`. Of the remaining variables, `r corVars` have weak correlation with the outcome variable, exceeding 15% in absolute value. All variables with strong pair-wise correlation have already been removed in the pre-processing stage.

##### 2.1 Clustering Analysis 

``` {r, Heirarchical_clustering, include=F}
data<-data.frame("classe"<-as.numeric(train$classe), train[,2:dim(train)[2]])
scale_train <- scale(data)
dist_train <- dist(scale_train, method="euclidean")
hclust_train <- hclust(dist_train, method="average")
cut_train <- cutree (hclust_train, k=5)
dend_obj <- as.dendrogram(hclust_train)
dend_col <- color_branches(dend_obj, h=5)
nodePar <- list(lab.cex=0.2,pch=c(NA,19), cex=0.5, col="blue")

hcm<-confusionMatrix(as.factor(train$classe),as.factor(cut_train))
format(hcm$table,big.mark=",") %>%kbl(row.names=T)%>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                      font_size = 10)%>%
        add_header_above(c("Prediction"=1,"Reference"=5))
```

``` {r, K-means_clustering, include=F}
kmc <- kmeans(train[,-1],centers=5)
cluster = kmc$cluster; outcome=train$classe 
correct = as.factor(cluster==train$classe)
density = data.frame(table(cluster,outcome))
df<-data.frame(cluster,outcome,correct)
df<-merge(df,density, by.x=c("cluster","outcome"), all.x=T); df$id<-row.names(df)

kmc<- confusionMatrix(as.factor(cluster),as.factor(train$classe))
```
Clustering techniques can be used to segregate outcomes based on all predictors for possible use in unsupervised learning models. Hierarchical and K-Means clustering results are shown in the appendices for reference. We note that hierarchical clustering delivers an accuracy of `r sprintf("%1.2f%%",hcm$overall[1]*100)` whereas K-means clustering delivers an accuracy of `r sprintf("%1.2f%%",kmc$overall[1]*100)`

##### 2.2 Principal Components Analysis 
To enhance the accuracy of models, we can transform the data into its principal components and train the model on this set. However, this procedure would greatly diminish the interpretation of the models. The table below shows the proportion of variance explained by each principal component vector in the data set (corresponding to row number), and we note that 18 components are needed to explain up to 90% of the variation in the data set.
```{r, Principal_Components_Analsyis, include=F}
pc_train<-prcomp(data,scale=T)
pg1<- fviz_eig(pc_train)
pg2<-fviz_pca_ind(pc_train,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
pg3<-fviz_pca_var(pc_train,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,     # Avoid text overlapping
             cex=0.3)

df<- data.frame(summary (pc_train)$importance)
df<-transpose(df)
names(df)<-c("Standard deviation", "Proportion of variance", 
             "Cumulative Proportion")

df%>%mutate_if(is.numeric,format,digits=2,nsmall=2)%>%kbl(row.names=T)%>%
        kable_styling(bootstrap_options = "basic")
```

### 3. Model Building and Selection  
We now proceed to build and compare several models to classify the outcome variable in the data set. 

##### 3.1. Cross-Validation Parameters 
In order to obtain the best assessment of out-of-sample performance for the models, a k-fold cross validation approach is set for all models. The k-fold cross validation method involves splitting the dataset into k-subsets. Each subset is held out while the model is trained on all other subsets. This process is completed until accuracy is determined for each instance in the dataset, and an overall accuracy estimate is provided. A 10-fold cross validation approach is used here.

```{r}
control <- trainControl(method="cv",number=10,verboseIter=F)
```
##### 3.2. Model Fitting 
We now proceed fo fit a number of machine learning models on the training data set. These include:

1. Decision Trees
2. Random Forests
3. Boosted Trees
4. Linear Discriminant Analysis
5. Regularized Regression - Lasso
6. Regularized Regression - Ridge

When fitting each model, accuracy is used as the primary assesment criteria of model performance since we are working with a classification problem. Accuracy is determined for in-sample (train) and out-of-sample (test) data. 
``` {r}
mod_tree<-train(classe~.,data=train,method="rpart",
                preProcess=c("BoxCox"), trControl = control, tuneLength=5)
pred_tree_is<-mod_tree%>%predict(train)
pred_tree_os<-mod_tree%>%predict(newdata=test)
acc_tree_is<-confusionMatrix(pred_tree_is,train$classe)$overall[1]
acc_tree_os<-confusionMatrix(pred_tree_os,test$classe)$overall[1]

```

```{r}
mod_rf<-train(classe~.,data=train,method="rf",
                preProcess=c("BoxCox"), trControl = control, tuneLength=5)
pred_rf_is<-mod_rf%>%predict(train)
pred_rf_os<-mod_rf%>%predict(newdata=test)
acc_rf_is<-confusionMatrix(pred_rf_is,train$classe)$overall[1]
acc_rf_os<-confusionMatrix(pred_rf_os,test$classe)$overall[1]

```

```{r}
mod_gbm<-train(classe~.,data=train,method="gbm",
                preProcess=c("BoxCox"),verbose=F)
pred_gbm_is<-mod_gbm%>%predict(train)
pred_gbm_os<-mod_gbm%>%predict(newdata=test)
acc_gbm_is<-confusionMatrix(pred_gbm_is, train$classe)$overall[1]
acc_gbm_os<-confusionMatrix(pred_gbm_os, test$classe)$overall[1]
```
 
```{r}
mod_lda<-train(classe~.,data=train,method="lda",
                preProcess=c("BoxCox"))
pred_lda_is<-mod_lda%>%predict(train)
pred_lda_os<-mod_lda%>%predict(newdata=test)
acc_lda_is<-confusionMatrix(pred_lda_is,train$classe)$overall[1]
acc_lda_os<-confusionMatrix(pred_lda_os,test$classe)$overall[1]
```

```{r}
mod_ridge<-train(classe~.,data=train,method="glmnet",
                preProcess=c("BoxCox"),
                tuneGrid = expand.grid(alpha=1, lambda=1))
pred_ridge_is<-mod_ridge%>%predict(train)
pred_ridge_os<-mod_ridge%>%predict(newdata=test)
acc_ridge_is<-confusionMatrix(pred_ridge_is,train$classe)$overall[1]
acc_ridge_os<-confusionMatrix(pred_ridge_os,test$classe)$overall[1]

mod_lasso<-train(classe~.,data=train,method="glmnet",
                preProcess=c("BoxCox"),
                tuneGrid = expand.grid(alpha=0, lambda=1))
pred_lasso_is<-mod_lasso%>%predict(train)
pred_lasso_os<-mod_lasso%>%predict(newdata=test)
acc_lasso_is<-confusionMatrix(pred_lasso_is,train$classe)$overall[1]
acc_lasso_os<-confusionMatrix(pred_lasso_os,test$classe)$overall[1]
```

##### 3.3. Model Performance Summary 

```{r}
df_models<-data.frame("Model" = c("Decision Trees", "Random Forests",
                                  "Linear Discriminant Analysis",
                                  "Regularized Regression - Ridge",
                                  "Regularized Regression - Lasso"),
                      "Accuracy - In-Sample" = c(acc_tree_is,acc_rf_is,
                                                  acc_lda_is,
                                                  acc_lasso_is,
                                                  acc_ridge_is),
                      "Accuracy - Out of Sample" = c(acc_tree_os,acc_rf_os,
                                                  acc_lda_os,
                                                  acc_lasso_os,
                                                  acc_ridge_os))
df_models%>%mutate_if(is.numeric,format,digits=2,nsmall=2)%>%kbl(row.names=T)%>%
        kable_styling(bootstrap_options = c("striped","hover", "condensed"),
                      font_size = 10)
```

The above table shows a summary of performance for each model. We can observe that the Random Forest model is able to achieve an in-sample accuracy of `r acc_rf_is` and out-of-sample accuracy of `r acc_rf_os` which significantly exceeds the performance of all other models. Secondly, the accuracy implies very high predictive power, due to which we do not need to consider further steps in developing a blended model.

To prepare the predicted classifications on the test data, the same transformations are carried out as before with the training data. 
``` {r}
test_clean<-test_raw[,-dim(test_raw)[2]]

# Remove metadata
test_clean<-test_clean[,-c(1:7)]

# Remove variables, standardize and recombine outcome variable
exclude <- unique(c(nzv,hcv,row.names(t_na)[na]))
covariates<- test_clean[,-which(names(test_clean)%in%exclude)]
preObj <- preProcess(covariates, method=c("BoxCox"))
std_cov <- predict (preObj, covariates)
test_clean <- data.frame(covariates)

pred_rf_test<-mod_rf%>%predict(newdata=test_clean)
levels(pred_rf_test) <- c("A","B","C","D","E")
df <- data.frame ("Problem ID" = test_raw[,160],
           "Predicted Class" = pred_rf_test)
df %>% kbl (align="c")%>% 
        kable_styling(bootstrap_options = c("striped","hover", "condensed"),
                      font_size = 10)
```


\newpage 
# Appendices 
### 1. Data Structure 
``` {r}
str (train_raw)
```
\newpage
### 2. Exploratory Analysis 

##### 2.1 Distribution of Individual Predictors 

``` {r, echo=F, results="hide", fig.keep="all"}
## Histograms
par (mfrow=c(6,6), mar=c(1,1,1,1))
lapply (1:dim(std_cov)[2], function(x) {
        hist (std_cov[,x], cex=0.3, breaks=25, col="salmon", 
                     main=names(std_cov)[x])
})
```

##### 2.2 QQ Plots 

``` {r, echo=F, results="hide", fig.keep="all"}
## Histograms
par (mfrow=c(6,6), mar=c(1,1,1,1))
lapply (1:dim(std_cov)[2], function(x) {
        qqnorm (std_cov[,x], cex=0.5, xlab=names(std_cov)[x])
})
```

##### 2.3 Correlation Plot 

``` {r, echo=F}
## Correlation matrix
corrplot(cor(train_n), method = "circle", type = "upper", 
         tl.cex = 0.3, tl.col = "darkgrey", diag=F)

corVars<- names(train_n)[abs(cor(train_n)[1,-1])>0.15]
```

##### 2.5 Heirarchical Clustering 

``` {r, echo=F}
plot (dend_col, horiz=T, nodePar=nodePar, leaflab="none")
```

##### 2.6 K-Means Clustering 

``` {r}
par (mfrow=c(1,1), mar=c(1,1,1,1))
df<-data.frame(cluster,outcome,correct)
df<-merge(df,density, by.x=c("cluster","outcome"), all.x=T); df$id<-row.names(df)
g <- ggplot (data=df,aes(x=cluster,y=outcome,size=Freq, col=correct)) +
        geom_point (alpha=0.1) +
        scale_size (range=c(.5,10), name="Frequency") +
        facet_wrap(.~correct)
g
```

##### 2.7 Principal Components Analysis 

``` {r} 
par (mfrow=c(1,1), mar=c(1,1,1,1))
gridExtra::grid.arrange(pg1,pg2,pg3,nrow=2)
```
